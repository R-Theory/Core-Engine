name: Performance Testing

on:
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
    paths:
      - 'backend/**'
      - 'frontend/**'
      - 'docker-compose.yml'
      - '.github/workflows/performance.yml'

jobs:
  # Backend API Performance Testing
  backend-performance:
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_USER: postgres
          POSTGRES_DB: core_engine_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'

    - name: Install dependencies
      working-directory: ./backend
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust pytest-benchmark

    - name: Start backend server
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql+asyncpg://postgres:test_password@localhost:5432/core_engine_test
        REDIS_URL: redis://localhost:6379/0
        SECRET_KEY: test-secret-key-for-ci
        ENVIRONMENT: testing
      run: |
        # Run database migrations
        alembic upgrade head

        # Start server in background
        uvicorn app.main:app --host 0.0.0.0 --port 8000 &
        sleep 10

        # Health check
        curl -f http://localhost:8000/health

    - name: Run API load tests with Locust
      working-directory: ./backend
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json

        class APIUser(HttpUser):
            wait_time = between(1, 3)

            def on_start(self):
                # Login to get auth token (if needed)
                pass

            @task(3)
            def health_check(self):
                self.client.get("/health")

            @task(2)
            def get_courses(self):
                self.client.get("/api/v1/courses")

            @task(1)
            def get_assignments(self):
                self.client.get("/api/v1/assignments")

            @task(1)
            def get_resources(self):
                self.client.get("/api/v1/resources")
        EOF

        # Run load test for 2 minutes with 10 concurrent users
        locust --headless --users 10 --spawn-rate 2 -H http://localhost:8000 --run-time 2m --csv=performance

    - name: Run database performance tests
      working-directory: ./backend
      env:
        DATABASE_URL: postgresql+asyncpg://postgres:test_password@localhost:5432/core_engine_test
        REDIS_URL: redis://localhost:6379/0
        SECRET_KEY: test-secret-key-for-ci
      run: |
        cat > test_db_performance.py << 'EOF'
        import pytest
        import asyncio
        import time
        from app.core.database import AsyncSessionLocal
        from sqlalchemy import text

        @pytest.mark.asyncio
        async def test_database_connection_performance():
            """Test database connection establishment time"""
            start_time = time.time()

            async with AsyncSessionLocal() as session:
                result = await session.execute(text("SELECT 1"))
                assert result.scalar() == 1

            connection_time = time.time() - start_time
            assert connection_time < 0.1  # Should connect within 100ms
            print(f"Database connection time: {connection_time:.3f}s")

        @pytest.mark.asyncio
        async def test_bulk_query_performance():
            """Test bulk query performance"""
            start_time = time.time()

            async with AsyncSessionLocal() as session:
                # Simulate bulk operations
                for _ in range(100):
                    await session.execute(text("SELECT 1"))

            query_time = time.time() - start_time
            assert query_time < 1.0  # 100 queries should complete within 1 second
            print(f"100 queries time: {query_time:.3f}s")
        EOF

        pytest test_db_performance.py -v

    - name: Archive performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: backend-performance-results
        path: |
          backend/performance_stats.csv
          backend/performance_stats_history.csv
          backend/performance_failures.csv
        retention-days: 30

  # Frontend Performance Testing
  frontend-performance:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Node.js 18
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Install dependencies
      working-directory: ./frontend
      run: |
        npm ci
        npm install -g lighthouse @lhci/cli

    - name: Build frontend
      working-directory: ./frontend
      env:
        NEXT_PUBLIC_API_URL: http://localhost:8000
        NODE_ENV: production
      run: |
        npm run build

    - name: Start frontend server
      working-directory: ./frontend
      run: |
        npm start &
        sleep 10

        # Wait for server to be ready
        for i in {1..30}; do
          if curl -f http://localhost:3000 > /dev/null 2>&1; then
            echo "Frontend server is ready"
            break
          fi
          echo "Waiting for frontend server... ($i/30)"
          sleep 2
        done

    - name: Run Lighthouse CI
      run: |
        cat > lighthouserc.json << 'EOF'
        {
          "ci": {
            "collect": {
              "url": ["http://localhost:3000"],
              "numberOfRuns": 3
            },
            "assert": {
              "assertions": {
                "categories:performance": ["warn", {"minScore": 0.8}],
                "categories:accessibility": ["error", {"minScore": 0.9}],
                "categories:best-practices": ["warn", {"minScore": 0.9}],
                "categories:seo": ["warn", {"minScore": 0.8}]
              }
            },
            "upload": {
              "target": "filesystem",
              "outputDir": "./lighthouse-results"
            }
          }
        }
        EOF

        lhci autorun || true

    - name: Bundle size analysis
      working-directory: ./frontend
      run: |
        # Analyze bundle size
        npm run build

        echo "=== Bundle Size Analysis ==="
        du -sh .next/static/chunks/* | sort -hr | head -20

        echo "=== Page Analysis ==="
        find .next -name "*.js" -exec du -sh {} \; | sort -hr | head -10

    - name: Archive frontend performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-performance-results
        path: |
          lighthouse-results/
          frontend/.next/static/
        retention-days: 30

  # End-to-End Integration Testing
  integration-tests:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Start full application stack
      run: |
        # Copy environment file for testing
        cp .env.example .env

        # Start the full stack
        docker-compose up -d

        # Wait for services to be healthy
        echo "Waiting for services to be ready..."
        for i in {1..60}; do
          if docker-compose exec -T backend curl -f http://localhost:8000/health > /dev/null 2>&1; then
            echo "Backend is ready"
            break
          fi
          echo "Waiting for backend... ($i/60)"
          sleep 5
        done

    - name: Run database migrations
      run: |
        docker-compose exec -T backend alembic upgrade head

    - name: Run integration tests
      run: |
        cat > integration_test.py << 'EOF'
        import requests
        import time
        import json

        def test_health_endpoints():
            """Test all health endpoints"""
            # Backend health
            response = requests.get("http://localhost:8000/health")
            assert response.status_code == 200
            print("✅ Backend health check passed")

            # Frontend health (might not be available in dev mode)
            try:
                response = requests.get("http://localhost:3000", timeout=5)
                print("✅ Frontend accessible")
            except:
                print("⚠️ Frontend not accessible (expected in some configurations)")

        def test_api_endpoints():
            """Test core API endpoints"""
            base_url = "http://localhost:8000/api/v1"

            # Test courses endpoint
            response = requests.get(f"{base_url}/courses")
            assert response.status_code in [200, 401]  # 401 if auth required
            print("✅ Courses endpoint accessible")

            # Test assignments endpoint
            response = requests.get(f"{base_url}/assignments")
            assert response.status_code in [200, 401]
            print("✅ Assignments endpoint accessible")

            # Test resources endpoint
            response = requests.get(f"{base_url}/resources")
            assert response.status_code in [200, 401]
            print("✅ Resources endpoint accessible")

        def test_database_connectivity():
            """Test database connectivity through API"""
            # This tests the full stack including database
            response = requests.get("http://localhost:8000/health")
            assert response.status_code == 200
            print("✅ Database connectivity verified")

        def test_redis_connectivity():
            """Test Redis connectivity"""
            # Redis is used for caching, this indirectly tests it
            response = requests.get("http://localhost:8000/health")
            assert response.status_code == 200
            print("✅ Redis connectivity verified")

        if __name__ == "__main__":
            print("Running integration tests...")
            test_health_endpoints()
            test_api_endpoints()
            test_database_connectivity()
            test_redis_connectivity()
            print("🎉 All integration tests passed!")
        EOF

        python integration_test.py

    - name: Test API response times
      run: |
        echo "Testing API response times..."

        # Test multiple endpoints for response time
        endpoints=(
          "http://localhost:8000/health"
          "http://localhost:8000/api/v1/courses"
          "http://localhost:8000/api/v1/assignments"
          "http://localhost:8000/api/v1/resources"
        )

        for endpoint in "${endpoints[@]}"; do
          echo "Testing $endpoint"

          # Use curl to measure response time
          response_time=$(curl -o /dev/null -s -w "%{time_total}" "$endpoint")

          echo "Response time: ${response_time}s"

          # Check if response time is reasonable (< 2 seconds)
          if (( $(echo "$response_time < 2.0" | bc -l) )); then
            echo "✅ Response time acceptable"
          else
            echo "⚠️ Response time high: ${response_time}s"
          fi
        done

    - name: Check container resource usage
      run: |
        echo "=== Container Resource Usage ==="
        docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}\t{{.MemPerc}}"

        echo "=== Container Logs (Last 50 lines) ==="
        docker-compose logs --tail=50

    - name: Cleanup
      if: always()
      run: |
        docker-compose down -v

  # Performance benchmarking against previous versions
  performance-regression:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history

    - name: Install performance testing tools
      run: |
        sudo apt-get update
        sudo apt-get install -y curl jq bc

    - name: Setup and test current version
      run: |
        echo "Testing current version performance..."

        # Setup current version
        cp .env.example .env
        docker-compose up -d

        # Wait for services
        sleep 30

        # Run migrations
        docker-compose exec -T backend alembic upgrade head

        # Test response times
        echo "Measuring current performance..."
        for i in {1..10}; do
          curl -o /dev/null -s -w "%{time_total}\n" "http://localhost:8000/health"
        done > current_times.txt

        # Calculate average
        current_avg=$(awk '{sum+=$1} END {print sum/NR}' current_times.txt)
        echo "Current average response time: ${current_avg}s"

        # Store result
        echo "$current_avg" > current_performance.txt

        # Cleanup
        docker-compose down -v

    - name: Compare with baseline
      run: |
        if [ -f current_performance.txt ]; then
          current=$(cat current_performance.txt)
          baseline="0.1"  # 100ms baseline - adjust as needed

          if (( $(echo "$current > $baseline * 2" | bc -l) )); then
            echo "⚠️ Performance regression detected!"
            echo "Current: ${current}s, Baseline: ${baseline}s"
            echo "Response time is more than 2x the baseline"
            # Don't fail the job, just warn
          else
            echo "✅ Performance within acceptable range"
            echo "Current: ${current}s, Baseline: ${baseline}s"
          fi
        fi